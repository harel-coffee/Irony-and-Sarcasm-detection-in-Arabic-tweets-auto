{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1614199020258,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "gt1_P6h9qNaa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import argparse\n",
    "import csv\n",
    "from scipy.sparse import *\n",
    "import pandas as pd\n",
    "import regex \n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1951,
     "status": "ok",
     "timestamp": 1614199022009,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "xUEUpiZiFdWp"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Concatenate,Conv1D,SpatialDropout1D,BatchNormalization,concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Activation, Permute, RepeatVector, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, GRU, Bidirectional, Input, Multiply\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1614199022011,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "9pbEdCxaSxO1"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4802,
     "status": "ok",
     "timestamp": 1614199027370,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "4BsKIFFjTSdD",
    "outputId": "50e0ca75-7fcb-4935-89fb-b0eb161a939d"
   },
   "outputs": [],
   "source": [
    "!pip install emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2554,
     "status": "ok",
     "timestamp": 1614199027370,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "gCZqqMdtTT0D"
   },
   "outputs": [],
   "source": [
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1614199027371,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Z3QUk5jYVQlh"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1614199027770,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "oCnVjZ2HV2Kc"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1614199028688,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "kTdWwlAoXJHz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1614199029749,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "wDfGX4OZX5S8"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1614199032369,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "B4yXt5sczMN_"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1614199060210,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "jq0f7KDzFWC8"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):  \n",
    "    # ref: https://github.com/bakrianoo/aravec\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
    "              \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
    "               \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
    "    \n",
    "    tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(tashkeel,\"\", text)\n",
    "    \n",
    "    # longation = re.compile(r'(.)\\1+')\n",
    "    # subst = r\"\\1\\1\"\n",
    "    # text = re.sub(longation, subst, text)\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    text = re.sub(r\"[a-zA-Z]\", '', text)\n",
    "    text = re.sub(r\"\\d+\", ' ', text)\n",
    "    text = re.sub(r\"\\n+\", ' ', text)\n",
    "    text = re.sub(r\"\\t+\", ' ', text)\n",
    "    text = re.sub(r\"\\r+\", ' ', text)\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1614199060209,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "0DwzoMAGIZez"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "def load_w2v(filepath,binary):\n",
    "    return KeyedVectors.load_word2vec_format(filepath, binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1579,
     "status": "ok",
     "timestamp": 1614199060211,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "x2Bz6byOQIcr"
   },
   "outputs": [],
   "source": [
    "# Get the vec representation of a set of tweets based on a specified embedding (can be a word or emoji mapping)\n",
    "def get_tweets_embeddings(tweets, vec_map, embedding_dim=100, init_unk=False, variance=None, weighted_average=True):\n",
    "    # Get the variance of the embedding map\n",
    "    if init_unk and variance is None:\n",
    "        variance = embedding_variance(vec_map)\n",
    "        print(\"Vector mappings have variance \", variance)\n",
    "    # If set, calculate the tf-idf weight of each embedding, otherwise, no weighting (all weights are 1.0)\n",
    "    if weighted_average:\n",
    "        weights = get_tf_idf_weights(tweets, vec_map)\n",
    "    else:\n",
    "        weights = {k: 1.0 for k in vec_map.vocab}\n",
    "    tw_emb = np.zeros((len(tweets), embedding_dim))\n",
    "    for i, tw in enumerate(tweets):\n",
    "        total_valid = 0\n",
    "        for word in tw.split():\n",
    "            embedding_vector=None \n",
    "            if word in vec_map.wv:\n",
    "              embedding_vector = vec_map[word]\n",
    "            if embedding_vector is not None:\n",
    "                tw_emb[i] = tw_emb[i] + embedding_vector * weights[word]\n",
    "                total_valid += 1\n",
    "            elif init_unk:\n",
    "                seed(1337603)\n",
    "                tw_emb[i] = np.random.uniform(-variance, variance, size=(1, embedding_dim))\n",
    "            # else:\n",
    "            #    print(\"Not found: \", word)\n",
    "        # Get the average embedding representation for this tweet\n",
    "        tw_emb[i] /= float(max(total_valid, 1))\n",
    "    return tw_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1614199061535,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "IwhvrY-FQVEA"
   },
   "outputs": [],
   "source": [
    "# Calculate the variance of an embedding (like glove, word2vec, emoji2vec, etc)\n",
    "# Used to sample new uniform distributions of vectors in the interval [-variance, variance]\n",
    "def embedding_variance(vec_map):\n",
    "    variance = np.sum([np.var(vec) for vec in vec_map.vectors]) / len(vec_map.vectors)\n",
    "    return variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1614199068585,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "RdDRmgcdXj1m"
   },
   "outputs": [],
   "source": [
    "# Get some idea about the max and mean length of the tweets (useful for deciding on the sequence length)\n",
    "def get_max_len_info(tweets, average=False):\n",
    "    sum_of_length = sum([len(l.split()) for l in tweets])\n",
    "    avg_tweet_len = sum_of_length / float(len(tweets))\n",
    "    print(\"Mean of train tweets: \", avg_tweet_len)\n",
    "    max_tweet_len = len(max(tweets, key=len).split())\n",
    "    print(\"Max tweet length is = \", max_tweet_len)\n",
    "    if average:\n",
    "        return avg_tweet_len\n",
    "    return max_tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1614199063661,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "5oAYpQTxQVXL"
   },
   "outputs": [],
   "source": [
    "# Extract each tweet's emojis - obv. it's just a brute force solution (so, it's slow) but works in ALL cases\n",
    "def extract_emojis(tweets):\n",
    "    emojis = []\n",
    "    for tw in tweets:\n",
    "        tw_emojis = []\n",
    "        for word in tw:\n",
    "            chars = list(word)\n",
    "            for ch in chars:\n",
    "                # print(ch)\n",
    "                if ch in emoji.UNICODE_EMOJI:\n",
    "                    tw_emojis.append(ch)\n",
    "        emojis.append(' '.join(tw_emojis))\n",
    "    return emojis\n",
    "\n",
    "def contain_emoji(tweets):\n",
    "\n",
    "  emoji___=''.join(c for c in tweets if c in emoji.UNICODE_EMOJI)  \n",
    "  if len(emoji___)>0:\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 796,
     "status": "ok",
     "timestamp": 1614199070514,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "JJLMqGyHXuH7"
   },
   "outputs": [],
   "source": [
    "def encode_text_as_word_indexes(train_tweets, test_tweets , max_num_words=None, lower=False, char_level=False):\n",
    "    # Create the tokenizer\n",
    "    tokenizer = Tokenizer(num_words=max_num_words, filters='', lower=lower, split=\" \", char_level=char_level)\n",
    "    # Fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_tweets)\n",
    "    # Encode each example as a sequence of word indexes based on the vocabulary of the tokenizer\n",
    "    x_train = tokenizer.texts_to_sequences(train_tweets)\n",
    "    x_test=tokenizer.texts_to_sequences(test_tweets)\n",
    "   \n",
    "    return tokenizer, x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1614199117185,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "gU7vy1XDvMY8"
   },
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    emoji___=''.join(c for c in text if c in emoji.UNICODE_EMOJI)\n",
    "    return emoji.demojize(emoji___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1614199118855,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "k2TifIQKyf6g"
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12174,
     "status": "ok",
     "timestamp": 1614199131423,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "gwkYUmCSrdbR"
   },
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \n",
    "    df[\"word_count\"] = df[\"cleaned_text\"].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    df[\"contain_emoji\"] = False\n",
    "    \n",
    "    df[\"demoji_text\"] = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "      df.loc[index, 'cleaned_text']=remove_urls(row[\"cleaned_text\"])\n",
    "      if contain_emoji(row[\"tweet\"]):\n",
    "        df.loc[index, 'contain_emoji']=True\n",
    "        df.loc[index, 'demoji_text']=convert_emojis(row[\"tweet\"])    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2610,
     "status": "ok",
     "timestamp": 1614199149090,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Ra6LXxb35uv7"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1614199132421,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "zC4pF5tqpsfz"
   },
   "outputs": [],
   "source": [
    "# Compute the word-embedding matrix\n",
    "def get_embedding_matrix(word2vec_map, word_to_index, embedding_dim, init_unk=True, variance=None):\n",
    "    # Get the variance of the embedding map\n",
    "    if init_unk and variance is None:\n",
    "        variance = embedding_variance(word2vec_map)\n",
    "        print(\"Word vectors have variance \", variance)\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors)\n",
    "    embedding_matrix = np.zeros((len(word_to_index) + 1, embedding_dim))\n",
    "    for word, i in word_to_index.items():\n",
    "        embedding_vector=None \n",
    "        if word in word2vec_map.wv:\n",
    "          embedding_vector = word2vec_map[word]\n",
    "\n",
    "        # embedding_vector = word2vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif init_unk:\n",
    "            # Unknown tokens are initialized randomly by sampling from a uniform distribution [-var, var]\n",
    "            seed(1337603)\n",
    "            embedding_matrix[i] = np.random.uniform(-variance, variance, size=(1, embedding_dim))\n",
    "        # else:\n",
    "        #    print(\"Not found: \", word)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xltj3tPwQrRd"
   },
   "outputs": [],
   "source": [
    "def keras_dl( word2vec_embegging,emoji_embedd,voacb_shape_embdd,input_shape_1,emoji_shape ):   \n",
    "    inp = Input(shape = (input_shape_1,), name = 'text_clean')\n",
    "    x = Embedding(voacb_shape_embdd,300,weights = [word2vec_embegging], trainable = False)(inp)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
    "    x_lstm_c1d = Conv1D(64,kernel_size=3,padding='valid',activation='tanh')(x_lstm)\n",
    "    x_lstm_c1d_gp = GlobalMaxPooling1D()(x_lstm_c1d)\n",
    "    \n",
    "    x_gru = Bidirectional(GRU(128, return_sequences = True))(x)\n",
    "    x_gru_c1d = Conv1D(64,kernel_size=2,padding='valid',activation='tanh')(x_gru)\n",
    "    x_gru_c1d_gp = GlobalMaxPooling1D()(x_gru_c1d)\n",
    "\n",
    "    inp2 = Input(shape = (emoji_shape,), name = 'emoji')\n",
    "    x2 = Embedding(voacb_shape_embdd,300,weights = [emoji_embedd], trainable = False)(inp2)\n",
    "    X2_conv1 = Conv1D(64, 5, kernel_initializer='he_normal', padding='valid', activation='relu')(x2)\n",
    "    X2_lstm = LSTM(32, kernel_initializer='he_normal', activation='tanh',\n",
    "             dropout=0.5, return_sequences=True)(X2_conv1)\n",
    "    x2_flatten = Flatten()(X2_lstm)\n",
    "    x2_dense =(Dense(16, activation='tanh') (x2_flatten))  \n",
    "    \n",
    "\n",
    "    \n",
    "    x_f = concatenate([x_lstm_c1d_gp, x_gru_c1d_gp])\n",
    "    x_f = BatchNormalization()(x_f)\n",
    "    x_f =(Dense(128, activation='tanh') (x_f))    \n",
    "    x_f = BatchNormalization()(x_f)\n",
    "    x_f = concatenate([x_f, x2_dense])    \n",
    "    x_f = (Dense(64, activation='tanh') (x_f))\n",
    "    x_f = Dense(3, activation = \"sigmoid\")(x_f)\n",
    "    model = Model(inputs = [inp, inp2], outputs = x_f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23940,
     "status": "ok",
     "timestamp": 1614199056351,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "fm34AVoVFRh1",
    "outputId": "6f403b46-bf01-416a-c447-5e488246d532"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1614199058448,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Lq4sJk9UFVYj"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1614199059729,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "LY4z4R1Mbfpe"
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1594,
     "status": "ok",
     "timestamp": 1614199072112,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "1CsBEDPlIBnK"
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_text'] = train_data.tweet.apply(clean_text)\n",
    "test_data['cleaned_text'] = test_data.tweet.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1706,
     "status": "ok",
     "timestamp": 1614199088300,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "7iSgilB8I1w7"
   },
   "outputs": [],
   "source": [
    "# Extract just the emojis present in each tweet in each set\n",
    "x_train_emoji = extract_emojis(train_data['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1614199089994,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "4Btei3rkb-hn"
   },
   "outputs": [],
   "source": [
    "x_test_emoji=extract_emojis(test_data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform(train_data)\n",
    "test_data = transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1614199102721,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "CcSfolWRPWYi"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(train_data['sentiment'])\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "train_labels_task2 = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1614199108900,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "-wC4ae7xPvU5"
   },
   "outputs": [],
   "source": [
    "integer_encoded = label_encoder.fit_transform(test_data['sentiment'])\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "test_labels_task2 = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1614199087259,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "S2DowlPQISV0"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300 #176 # based on our inputs; TODO: remove outliers? dynamically calculate!\n",
    "MAX_NB_WORDS = len(word2vec_map.vocab)\n",
    "EMBEDDING_DIM = 300 # w2v, fastText; GloVe=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3591,
     "status": "ok",
     "timestamp": 1614199085452,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Lt4MCMAfIc6l"
   },
   "outputs": [],
   "source": [
    "word2vec_map = load_w2v(\"/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/arabic-news.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1614199097392,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "1CPPtHHeJE8T"
   },
   "outputs": [],
   "source": [
    "emoji2vec_map = load_w2v(\"/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/emoji2vec.bin\",True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1614199102448,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "pqY6PWFpPVLa"
   },
   "source": [
    "#### Prepare data by transforming vector2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2293,
     "status": "ok",
     "timestamp": 1614199117184,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "QAIwvumPSb73",
    "outputId": "9ff30500-0c00-44e7-ca34-04d2b563c152"
   },
   "outputs": [],
   "source": [
    "# Get the max length of the train tweets\n",
    "max_tweet_length = train_data.cleaned_text.map(len).max()\n",
    "\n",
    "# Convert all tweets into sequences of word indices\n",
    "tokenizer, train_indices,test_indices = encode_text_as_word_indexes(train_data['cleaned_text'],test_data['cleaned_text'],  lower=True)\n",
    "vocab_size = len(tokenizer.word_counts) + 1\n",
    "word_to_index = tokenizer.word_index\n",
    "print(\"There are %s unique tokens.\" % len(word_to_index))\n",
    "\n",
    "# Pad sequences with 0s (can do it post or pre - post works better here)\n",
    "x_train = pad_sequences(train_indices, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "x_test = pad_sequences(test_indices, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "\n",
    "# Get the word to index and the index to word mappings\n",
    "word_index = tokenizer.word_index\n",
    "index_to_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Set the sequence length\n",
    "SEQUENCE_LENGTH = max_tweet_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8538,
     "status": "ok",
     "timestamp": 1614199131425,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "Q2UQgeC42vRq",
    "outputId": "9bde87c5-934b-4e0b-af95-9f9de0f15287"
   },
   "outputs": [],
   "source": [
    "# Convert all emoji into sequences of word indices\n",
    "tokenizer_emji, train_indices_emji,test_indices_emji = encode_text_as_word_indexes(train_data['demoji_text'],test_data['demoji_text'],  lower=True)\n",
    "word_to_index_emji= tokenizer_emji.word_index\n",
    "print(\"There are %s unique tokens.\" % len(word_to_index_emji))\n",
    "\n",
    "# Pad sequences with 0s (can do it post or pre - post works better here)\n",
    "x_train_emji = pad_sequences(train_indices_emji, maxlen=train_data.demoji_text.map(len).max(), padding=\"post\", truncating=\"post\", value=0.)\n",
    "x_test_emji = pad_sequences(test_indices_emji, maxlen=276, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "\n",
    "# Get the word to index and the index to word mappings\n",
    "word_index_emji = tokenizer_emji.word_index\n",
    "index_to_word_emji = {index: word for word, index in word_index_emji.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyr-oqh_Zy4d"
   },
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6659,
     "status": "ok",
     "timestamp": 1611066169337,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "QlAt8UjEs3uU",
    "outputId": "f7ec95d7-f021-423f-9094-bd14ef072516"
   },
   "outputs": [],
   "source": [
    "w2v_embedd=get_embedding_matrix(word2vec_map,word_index,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2762,
     "status": "ok",
     "timestamp": 1611066172122,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "iKYStjiltQ5D",
    "outputId": "0f3f0a8c-f661-40c5-bc09-22b08abfe292"
   },
   "outputs": [],
   "source": [
    "emoji_embedd=get_embedding_matrix(emoji2vec_map,word_index,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWZNylYdRAAw"
   },
   "outputs": [],
   "source": [
    "model=keras_dl(w2v_embedd,emoji_embedd,w2v_embedd.shape[0],max_tweet_length,train_data.demoji_text.map(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5dfXtC1AzpM"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "602Y9_63uar4"
   },
   "outputs": [],
   "source": [
    "my_optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.99, decay=0.01)\n",
    "reduceLR = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju--8UlvMsa8"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk8LJeM5GwWn",
    "outputId": "8aa90ef2-de2e-405a-a3d3-368383244662"
   },
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "history = model.fit([x_train,x_train_emji], train_labels_task2, batch_size=32, epochs=500, shuffle=True,\n",
    "                             validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11784,
     "status": "ok",
     "timestamp": 1614199197574,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "6KK9GYkutHCG"
   },
   "outputs": [],
   "source": [
    "prediction=model.predict([x_test,x_test_emji])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1614199206449,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "s5jaSNMexRaJ"
   },
   "outputs": [],
   "source": [
    "argsorted_y = np.argsort(prediction)[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1614199785752,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "-xS24LI0zdaj"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels_task2.argmax(axis=1), argsorted_y)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 866,
     "status": "ok",
     "timestamp": 1614199830406,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "FzBmp7h7zoss",
    "outputId": "3dde9b94-6423-4cd1-cf45-32a0b00707f0"
   },
   "outputs": [],
   "source": [
    "np.mean(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1614199827653,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "iErpnO8Bzqb2",
    "outputId": "ce5bf97d-9271-42ee-e986-a042b5ac184a"
   },
   "outputs": [],
   "source": [
    "np.mean(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1614199255713,
     "user": {
      "displayName": "Reem Elsayed",
      "photoUrl": "",
      "userId": "13751898496823085405"
     },
     "user_tz": -120
    },
    "id": "4_ThtYusxZNa",
    "outputId": "a90c0420-4d72-4a83-8728-520a43fad74c"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels_task2.argmax(axis=1), argsorted_y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRmCSH20yFIvU89BPetUel",
   "collapsed_sections": [
    "k8gpz2mVr1a4"
   ],
   "name": "Sarcasm-Detection-Multiple-DL-approach-task2-v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
